Dysarthria, a motor speech disorder resulting from neurological injury, significantly affects an individual's ability to communicate, impacting their speech clarity and intelligibility. Detecting and evaluating dysarthria through computational methods can provide substantial benefits in clinical settings and rehabilitative applications. This study introduces a novel approach using a hybrid deep learning architecture, combining Convolutional Neural Networks (CNNs), Bidirectional Long Short-Term Memory (BiLSTM), and Gated Recurrent Units (GRU) to analyze and classify audio signals of speech for the detection of dysarthria. By leveraging the strengths of CNNs in feature extraction from raw audio signals, and the sequential data processing capabilities of BiLSTM and GRU layers, our model effectively captures both the temporal dynamics and intricate nuances in speech that are indicative of dysarthria. The architecture is evaluated on a dataset composed of speech samples from both dysarthric speakers and healthy controls, processed into spectrogram images to retain rich temporal and frequency information. Initial results demonstrate promising performance, with our model achieving an accuracy of 97.4% in distinguishing between dysarthric and non-dysarthric speech samples. Further, the model shows robustness in handling variabilities in speech severity and different types of dysarthria. These findings suggest that our CNN-BiLSTM-GRU model could serve as a powerful tool for the automated detection and assessment of dysarthria, potentially assisting speech therapists in diagnosis and treatment planning.
